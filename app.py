"""
## Setup

To install the dependencies for this script, run:

```
brew install portaudio
brew install espeak-ng
uv sync
```

## API key

Ensure the `GOOGLE_API_KEY` environment variable is set to the api-key
you obtained from Google AI Studio.

## Run

To run the script:

```
python Get_started_LiveAPI_NativeAudio.py
```

Start talking to Gemini
"""

import asyncio
import os
import sys
import traceback

import dotenv
import numpy as np
import pyaudio
from google import genai
from google.genai import types
from rich.console import Console
from rich.markdown import Markdown

if sys.version_info < (3, 11, 0):
    import exceptiongroup
    import taskgroup

    asyncio.TaskGroup = taskgroup.TaskGroup
    asyncio.ExceptionGroup = exceptiongroup.ExceptionGroup

dotenv.load_dotenv()

log_console = Console()

# åŸºç¡€é…ç½®
FORMAT = pyaudio.paInt16
CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024

# éŸ³é‡ç›‘æµ‹é…ç½®
VOLUME_WINDOW_SIZE = 5  # å¹³æ»‘çª—å£å¤§å°
VOLUME_THRESHOLD_RATIO = 0.1  # è‡ªé€‚åº”é˜ˆå€¼æ¯”ä¾‹
MIN_VOLUME_THRESHOLD = 100  # æœ€å°é˜ˆå€¼
MAX_VOLUME_THRESHOLD = 2000  # æœ€å¤§é˜ˆå€¼
DEBUG_VOLUME = False  # æ˜¯å¦æ˜¾ç¤ºéŸ³é‡è°ƒè¯•ä¿¡æ¯

# è¯­éŸ³è®¾ç½®
pya = pyaudio.PyAudio()
# ä¸»é¢˜å’Œåœºæ™¯å®šä¹‰
THEMES = {
    "business": ["job interview", "business meeting", "presentation", "networking"],
    "travel": ["airport", "hotel", "restaurant", "sightseeing"],
    "daily life": ["shopping", "weather", "hobbies", "family"],
    "social": ["meeting friends", "party", "social media", "dating"],
}

client = genai.Client(api_key=os.environ["GOOGLE_API_KEY"])  # GOOGLE_API_KEY must be set as env variable

MODEL = "gemini-2.0-flash-live-001"
CONFIG = {"response_modalities": ["AUDIO"]}
CONFIG_TEXT = {"response_modalities": ["TEXT"]}


class AudioLoop:
    def __init__(self):
        self.live_session = None
        self.text_session = None

        self.audio_out_queue = None
        self.audio_in_queue = None

        self.initialized = False

        self.paused = False
        self.running_step = 0

        self.current_theme = None
        self.current_scenario = None

        self.audio_stream = None
        self.receive_audio_task = None
        self.play_audio_task = None
        self.voice_client = None

        # éŸ³é‡ç›‘æµ‹ç›¸å…³
        self.volume_history = []  # éŸ³é‡å†å²è®°å½•
        self.adaptive_threshold = MIN_VOLUME_THRESHOLD  # è‡ªé€‚åº”é˜ˆå€¼
        self.speaking_detected = False  # æ˜¯å¦æ£€æµ‹åˆ°è¯´è¯

    def calculate_volume(self, audio_data: bytes) -> float:
        """è®¡ç®—éŸ³é¢‘éŸ³é‡ - ä½¿ç”¨RMSæ–¹æ³•ï¼Œå¯¹å™ªå£°æ›´é²æ£’ï¼Œèƒ½æ›´å‡†ç¡®åœ°åæ˜ å®é™…éŸ³é‡"""
        try:
            # å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„
            audio_array = np.frombuffer(audio_data, dtype=np.int16)
            # ä½¿ç”¨RMS (Root Mean Square) è®¡ç®—éŸ³é‡ï¼Œå¯¹å™ªå£°æ›´é²æ£’
            rms = np.sqrt(np.mean(audio_array.astype(np.float32) ** 2))
            return rms
        except Exception as e:
            log_console.print(f"éŸ³é‡è®¡ç®—é”™è¯¯: {e}", style="red")
            return 0

    def update_adaptive_threshold(self, current_volume: float):
        """æ›´æ–°è‡ªé€‚åº”é˜ˆå€¼"""
        # æ·»åŠ å½“å‰éŸ³é‡åˆ°å†å²è®°å½•
        self.volume_history.append(current_volume)

        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.volume_history) > VOLUME_WINDOW_SIZE * 2:
            self.volume_history = self.volume_history[-VOLUME_WINDOW_SIZE * 2 :]

        # è®¡ç®—èƒŒæ™¯å™ªå£°æ°´å¹³ï¼ˆä½¿ç”¨å†å²è®°å½•çš„ä¸­ä½æ•°ï¼‰
        if len(self.volume_history) >= VOLUME_WINDOW_SIZE:
            background_noise = np.median(self.volume_history[-VOLUME_WINDOW_SIZE:])
            # è‡ªé€‚åº”é˜ˆå€¼ = èƒŒæ™¯å™ªå£° + æ¯”ä¾‹ç³»æ•°
            new_threshold = background_noise + (background_noise * VOLUME_THRESHOLD_RATIO)
            # é™åˆ¶é˜ˆå€¼èŒƒå›´
            self.adaptive_threshold = max(MIN_VOLUME_THRESHOLD, min(MAX_VOLUME_THRESHOLD, new_threshold))

    def detect_speaking(self, volume: float) -> bool:
        """æ£€æµ‹æ˜¯å¦åœ¨è¯´è¯"""
        # æ›´æ–°è‡ªé€‚åº”é˜ˆå€¼
        self.update_adaptive_threshold(volume)

        # æ£€æµ‹è¯´è¯çŠ¶æ€
        is_speaking = volume > self.adaptive_threshold

        # æ·»åŠ ç®€å•çš„çŠ¶æ€å¹³æ»‘ï¼Œé¿å…é¢‘ç¹åˆ‡æ¢
        if is_speaking and not self.speaking_detected:
            self.speaking_detected = True
        elif not is_speaking and self.speaking_detected:
            # å»¶è¿Ÿå…³é—­ï¼Œé¿å…è¯´è¯é—´éš™çš„é—ªçƒ
            self.speaking_detected = False

        return self.speaking_detected

    def calculate_pronunciation_score(self, audio_data):
        """è®¡ç®—å‘éŸ³å¾—åˆ†"""
        try:
            audio_array = np.frombuffer(audio_data, dtype=np.int16)

            # è®¡ç®—éŸ³é¢‘ç‰¹å¾
            energy = np.mean(np.abs(audio_array))
            zero_crossings = np.sum(np.abs(np.diff(np.signbit(audio_array))))

            # å½’ä¸€åŒ–å¹¶è®¡ç®—å¾—åˆ†
            energy_score = min(100, energy / 1000)
            rhythm_score = min(100, zero_crossings / 100)

            # æœ€ç»ˆå¾—åˆ†
            final_score = int(0.6 * energy_score + 0.4 * rhythm_score)
            return min(100, max(0, final_score))
        except Exception as e:
            log_console.print(f"è¯„åˆ†è®¡ç®—é”™è¯¯: {e}", style="red")
            return 70  # å‡ºé”™æ—¶è¿”å›é»˜è®¤åˆ†æ•°

    # @staticmethod
    # def read_audio_file(file_path: str) -> pyaudio.Stream:
    #     try:
    #         import wave

    #         with wave.open(file_path, "rb") as wf:
    #             stream = pya.open(format=pya.get_format_from_width(wf.getsampwidth()), channels=wf.getnchannels(), rate=wf.getframerate(), output=True)
    #             CHUNK = 1024
    #             data = wf.readframes(CHUNK)
    #             while data:
    #                 stream.write(data)
    #                 data = wf.readframes(CHUNK)

    #             log_console.print(f"âœ… æˆåŠŸä½¿ç”¨pyaudioè¯»å–éŸ³é¢‘æ–‡ä»¶: {file_path}", style="green")
    #             return stream
    #     except FileNotFoundError:
    #         log_console.print(f"âŒ éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}", style="red")
    #         return None
    #     except Exception as e:
    #         log_console.print(f"âŒ ä½¿ç”¨pyaudioè¯»å–éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}", style="red")
    #         return None

    async def startup(self):
        await self.text_session.send_client_content(
            turns=types.Content(
                role="user",
                parts=[
                    types.Part(
                        text="""
## Role & Persona
You are "Echo," a professional, patient, and encouraging English speaking coach. Your goal is to create a supportive and effective learning environment for the user. Always maintain a positive and friendly tone.

## Core Directives
1.  **Grammar & Pronunciation Correction:**
    *   For pronunciation, identify specific mispronounced words. Use the International Phonetic Alphabet (IPA) to show the difference between the user's pronunciation and the correct one where helpful.
    *   For grammar, clearly state the error and explain the correct structure or rule.
2.  **Scoring:** Provide a pronunciation score from 0 to 100 after each user utterance. The score should reflect accuracy, fluency, and clarity.
3.  **Control Commands:** You must understand and act upon these commands:
    *   When the user says "Can I have a break," you will pause the session by saying, "Of course, take your time. Just say 'OK, let's continue' when you're ready."
    *   When the user says "OK, let's continue," you will resume by providing the next practice sentence.
4.  **Contextual Practice:** The practice sentences you provide should be logically connected to build a mini-dialogue within the chosen theme.

## Interaction Flow
1.  **Initial Greeting:** Start the conversation by introducing yourself as Echo and asking the user which theme they would like to practice. The themes are: Business, Travel, Daily Life, or Social.
2.  **User Practice:** The user will speak a sentence in English.
3.  **Your Feedback Loop:** For each sentence the user speaks, you must provide a response that strictly follows the **Strict Output Format** defined below.
4.  **Continuation:** After providing feedback and the next sentence, wait for the user's next attempt. This loop continues until the user ends the session.

## Strict Output Format
You MUST use the following Markdown format for every feedback response. Do not deviate from it.

```
**Your Utterance:** [The user's transcribed sentence here]
**Pronunciation Score:** [Score]/100

**Feedback:**
*   **Pronunciation:** [Detailed feedback on pronunciation. Mention specific words. Use IPA if necessary. e.g., The word "to" in "want to" was a bit harsh. In natural speech, it often softens to a "tÉ™" sound /tÉ™/.]
*   **Grammar:** [Detailed feedback on grammar. e.g., The structure "want go" is incorrect. The verb "want" should be followed by an infinitive "to + verb".]

**Suggestion:**
*   [Provide a corrected version of the user's sentence. e.g., "I want to go to the airport."]
*   [Offer a tip for improvement. e.g., "Remember to place 'to' between 'want' and another verb."]

**Next Practice Sentence:**
*   [Provide the next logical sentence for the user to practice. e.g., "Now, try saying this: How can I get there?"]
```

## Example Interaction

**User says:** "I want go to the airport."

**Your expected response:**

**Your Utterance:** I want go to the airport.
**Pronunciation Score:** 85/100

**Feedback:**
*   **Pronunciation:** Your pronunciation was very clear. One small tip: the word "airport" has two syllables, "air-port" /ËˆÉ›É™rËŒpÉ”Ërt/. Make sure to pronounce both parts distinctly.
*   **Grammar:** The structure "want go" is a common error. The verb "want" needs to be followed by the infinitive form, which is "to go".

**Suggestion:**
*   The correct sentence is: "I want **to go** to the airport."
*   Try to remember the pattern "want + to + [verb]".

**Next Practice Sentence:**
*   Now, try saying this: "Could you tell me which terminal is for international flights?"

---

If you understand all the instructions, your role, and the required format, please respond with "OK" in English as specified in the Core Directives.
"""
                    )
                ],
            ),
            turn_complete=True,
        )
        current_response = []
        async for raw_response in self.text_session.receive():
            if raw_response.text:
                current_response.append(raw_response.text)
                if "".join(current_response).startswith("OK"):
                    self.initialized = True
                    return

    async def listen_audio(self):
        """ç›‘å¬éŸ³é¢‘è¾“å…¥"""
        mic_info = pya.get_default_input_device_info()
        self.audio_stream = await asyncio.to_thread(
            pya.open,
            format=FORMAT,
            channels=CHANNELS,
            rate=SEND_SAMPLE_RATE,
            input=True,
            input_device_index=mic_info["index"],
            frames_per_buffer=CHUNK_SIZE,
        )
        if __debug__:
            kwargs = {"exception_on_overflow": False}
        else:
            kwargs = {}

        log_console.print("ğŸ¤ è¯·è¯´è‹±è¯­", style="yellow")
        while True:
            if self.paused:
                await asyncio.sleep(0.1)
                continue

            data = await asyncio.to_thread(self.audio_stream.read, CHUNK_SIZE, **kwargs)
            if self.running_step > 1:
                continue

            # éŸ³é‡æ£€æµ‹
            volume = self.calculate_volume(data)
            if volume == 0:
                log_console.print("ğŸ¤ :", style="yellow", end="")
                continue
            is_speaking = self.detect_speaking(volume)
            # è°ƒè¯•ä¿¡æ¯
            if DEBUG_VOLUME:
                log_console.print(f"\réŸ³é‡: {volume:.1f}, é˜ˆå€¼: {self.adaptive_threshold:.1f}, è¯´è¯: {is_speaking}", style="dim", end="")

            if is_speaking:
                if self.running_step == 0:
                    log_console.print("ğŸ¤ :", style="yellow", end="")
                    self.running_step += 1
                log_console.print("*", style="green", end="")
            elif self.running_step > 0:
                # è¯´è¯åœæ­¢ï¼Œé‡ç½®çŠ¶æ€
                self.running_step = 0
                log_console.print()  # æ¢è¡Œ

            await self.audio_out_queue.put({"data": data, "mime_type": "audio/pcm"})

    async def send_realtime(self):
        """å‘é€éŸ³é¢‘æ•°æ®"""
        while True:
            if self.paused:
                await asyncio.sleep(0.1)
                continue

            msg = await self.audio_out_queue.get()
            await self.live_session.send_realtime_input(audio=msg)

    async def receive_audio(self):
        """æ¥æ”¶éŸ³é¢‘æ•°æ®å¹¶å¤„ç†"""
        while True:
            turn = self.live_session.receive()
            async for response in turn:
                if self.running_step == 1:
                    log_console.print("\nâ™»ï¸ å¤„ç†ä¸­ï¼š", end="")
                    self.running_step += 1

                if data := response.data:
                    self.audio_in_queue.put_nowait(data)
                    continue
                if text := response.text:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ§åˆ¶å‘½ä»¤
                    if "can i have a break" in text.lower():
                        self.paused = True
                        log_console.print("\nâ¸ï¸ ä¼šè¯å·²æš‚åœã€‚è¯´ 'OK let's continue' ç»§ç»­", style="yellow")
                    elif "ok let's continue" in text.lower() and self.paused:
                        self.paused = False
                        log_console.print("\nâ–¶ï¸ ä¼šè¯ç»§ç»­", style="green")

                    # æ˜¾ç¤ºå“åº”
                    log_console.print("\nğŸ¤– =============================================", style="yellow")
                    log_console.print(Markdown(text))

                self.running_step = 0 if not self.paused else 2

            # If you interrupt the model, it sends a turn_complete.
            # For interruptions to work, we need to stop playback.
            # So empty out the audio queue because it may have loaded
            # much more audio than has played yet.
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()

    async def play_audio(self):
        stream = await asyncio.to_thread(
            pya.open,
            format=FORMAT,
            channels=CHANNELS,
            rate=RECEIVE_SAMPLE_RATE,
            output=True,
        )
        while True:
            bytestream = await self.audio_in_queue.get()
            log_console.print("ğŸ™ å£°éŸ³æ’­æ”¾ä¸­........", style="yellow")
            await asyncio.to_thread(stream.write, bytestream)
            log_console.print("ğŸ™ æ’­æ”¾å®Œæ¯•", style="green")

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as live_session,
                client.aio.live.connect(model=MODEL, config=CONFIG_TEXT) as text_session,
                asyncio.TaskGroup() as tg,
            ):
                self.live_session = live_session
                self.text_session = text_session

                log_console.print("Gemini è‹±è¯­å£è¯­åŠ©æ‰‹", style="green", highlight=True)
                log_console.print("Make by Adam Zhou: X@summychou", style="blue")
                log_console.print("============================================", style="yellow")

                await self.startup()
                if not self.initialized:
                    log_console.print("åˆå§‹åŒ–å¤±è´¥ âŒ", style="red")
                    return
                log_console.print("åˆå§‹åŒ–å®Œæˆ âœ…", style="green")
                log_console.print("è¯·å¼€å§‹ä½ çš„è¡¨æ¼”", style="yellow")

                self.audio_out_queue = asyncio.Queue(maxsize=5)
                self.audio_in_queue = asyncio.Queue()

                tg.create_task(self.listen_audio())
                tg.create_task(self.send_realtime())
                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                def check_error(task):
                    if task.cancelled():
                        return
                    if task.exception():
                        log_console.print(f"Error: {task.exception()}", style="red")
                        sys.exit(-1)

                for task in tg._tasks:
                    task.add_done_callback(check_error)

        except asyncio.CancelledError:
            pass
        except asyncio.ExceptionGroup as EG:
            if self.audio_stream:
                self.audio_stream.close()
            traceback.print_exception(EG)


if __name__ == "__main__":
    loop = AudioLoop()
    asyncio.run(loop.run())
